import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
import string

from snowballstemmer import PorterStemmer

df = pd.read_csv('examples.csv', header=0)

def get_tokens(x):
    punc = string.punctuation[:5] + 'â€”' + string.punctuation[5:]
    x_without_punc = ''.join(s for s in x if s not in punc)
    x_tokenize = word_tokenize(x_without_punc)
    stop = set(stopwords.words('russian'))
    x_without_stop = [word for word in x_tokenize if word.lower() not in stop]
    stemmer = SnowballStemmer("russian")
    x_stemm = [stemmer.stem(word) for word in x_without_stop]
    print(x_stemm)
    return x_stemm

def get_tf_idf():
    vectorizer = TfidfVectorizer(tokenizer=get_tokens)
    tdm = vectorizer.fit_transform(df['text'])
    features = vectorizer.get_feature_names()
    print(tdm)
    print(features)

get_tf_idf()

