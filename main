import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

df = pd.read_csv('articles.csv', header=0)

def tokenize_articles(df):
    df['title'] = df['title'].astype(str)
    df['text'] = df['text'].astype(str)

    #deleting punctuation symbols
    punc = string.punctuation[:5] + 'â€”' + string.punctuation[5:]

    df['cleaned_title'] = df['title'].apply(lambda row: (''.join(ch for ch in row if ch not in punc).lower()))
    df['cleaned_text'] = df['text'].apply(lambda row: (''.join(ch for ch in row if ch not in punc).lower()))

    #apply tokenization
    df['tokenized_title'] = df.apply(lambda row: word_tokenize(row['cleaned_title']), axis=1)
    df['tokenized_text'] = df.apply(lambda row: word_tokenize(row['cleaned_text']), axis=1)

    #remove stopwords
    stop = set(stopwords.words('russian'))
    df['tokenized_text'] = df.apply(lambda row: [w for w in row['tokenized_text'] if w not in stop], axis=1)



tokenize_articles(df)